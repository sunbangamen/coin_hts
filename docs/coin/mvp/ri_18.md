# Issue #29: Phase 3 운영 보완 - 요약 및 해결 계획

## 1. 이슈 정보 수집

### 기본 정보
- **이슈 번호**: #29
- **제목**: [Phase 3] 운영 보완 - 비동기 처리, 외부 스토리지, 성능 최적화
- **상태**: OPEN
- **담당자**: 미지정
- **라벨**: 없음
- **URL**: https://github.com/sunbangamen/coin_hts/issues/29

### 핵심 메타데이터
- **마감 기한**: 4주 (Week 1-4)
- **현재 pytest 상태**: 167/187 통과 (89.3%)
- **기반 작업**: Phase 0-2 완료, Issue #27 완료
- **알려진 이슈**: 5건 (High 3건, Medium 1건, Low 1건)

---

## 2. 문제 이해

### 핵심 요구사항
Phase 3는 **운영 안정성 확보**를 목표로 하며, Phase 1/2에서 구축한 백테스팅 시스템을 실제 운영 환경에서 사용할 수 있도록 다음 영역을 보완합니다:

1. **성능 최적화** (HIGH)
   - VolumeZoneBreakout 전략이 300캔들 이상에서 SLA(< 1초) 근접 또는 초과 (1.44~5.17초)
   - 증분 윈도우 최적화 재검증 필요

2. **비동기 태스크 처리** (HIGH)
   - 장시간 실행 백테스트(1000캔들+)에서 HTTP 타임아웃 발생
   - Celery/RQ 기반 비동기 API 구현 필요

3. **포지션 관리** (HIGH)
   - 백테스트 결과에 진입/청산 가격, 손익 계산 등 상세 정보 부족
   - Position 모델 및 프론트엔드 렌더링 필요

4. **외부 스토리지 연동** (MEDIUM)
   - 로컬 볼륨 의존성 제거
   - S3/NFS/OneDrive 중 1개 이상 연동

5. **결과 저장 개선** (MEDIUM)
   - `index.json` 동시 접근 문제
   - JSON 파일 크기 증가 → PostgreSQL + Parquet 전환

6. **운영 가이드** (LOW)
   - README, 트러블슈팅, 마이그레이션 체크리스트 작성

7. **자동 백업 및 모니터링** (LOW)
   - 데이터 백업 자동화
   - 구조화된 로깅 및 알림 시스템

### 제약사항
- ✅ Phase 1/2 기존 기능 유지 (Breaking Change 최소화)
- ✅ Docker Compose 기반 아키텍처 유지
- ✅ pytest 통과율 167/187 유지 또는 개선 (목표: 187/187)
- ⚠️ VolumeZoneBreakout 성능 병목 재검증 필요

### 현재 상태
- **완료**: Phase 0-2, Issue #27 검증 완료
- **미완료**: 비동기 API 3개 테스트 실패, 포지션 관리 5개 테스트 실패, 결과 저장 3개 테스트 실패
- **리스크**: VolumeZoneBreakout 전략 성능 저하 (300캔들: 1.44초, 1000캔들: 5.17초)

---

## 3. 해결 계획 수립

Issue #29의 본문에 **매우 상세한 8단계 Task 플랜**이 이미 작성되어 있습니다. 이를 기반으로 요약된 실행 계획은 다음과 같습니다:

### 📋 Task 요약

| Task | 설명 | 우선순위 | 예상 시간 | 주차 |
|------|------|---------|----------|------|
| **3.1** | VolumeZoneBreakout 성능 재검증 및 최적화 | HIGH | 3-4일 | Week 1 |
| **3.2** | 비동기 백테스트 API 구현 (Celery + Redis) | HIGH | 3-4일 | Week 1 |
| **3.3** | 포지션 관리 기능 구현 (Position 모델, Frontend) | HIGH | 2-3일 | Week 2 |
| **3.4** | 외부 스토리지 연동 (S3 권장) | MEDIUM | 3-4일 | Week 2-3 |
| **3.5** | 결과 저장 개선 (PostgreSQL + Parquet) | MEDIUM | 2일 | Week 3 |
| **3.6** | 운영 가이드 작성 (README, 트러블슈팅) | LOW | 2일 | Week 3 |
| **3.7** | 자동 백업 및 모니터링 | LOW | 2-3일 | Week 4 |
| **3.8** | 통합 테스트 및 Phase 3 완료 리포트 | CRITICAL | 1일 | Week 4 |

### 🎯 주요 성공 기준
- ✅ **pytest 통과율**: Phase 2 대비 회귀 방지를 위해 `167/187`을 최소선으로 유지, Phase 3 진행 KPI `≥180/187`, 최종 인수 기준 `187/187`.
- ✅ **성능 SLA 준수**: 운영 SLA(응답 1초 미만)를 3개 구간으로 세분화.
  - 100캔들: < 0.1초 (트레이더 상호작용 지연 방지) **[✅ 실제: 0.0228초]**
  - 300캔들: < 0.5초 (주 사용 구간) **[✅ 실제: 0.0708초]**
  - 1000캔들: < 1.0초 (리포트 생성 한계) **[✅ 실제: 0.2688초]**
  - SLA 미달 시 Phase 2 수준 유지 + 성능 개선 안을 리포트에 명시.
- ✅ **비동기 API 품질**: 제출/조회/취소 API 모두 99% < 2초, 실패 태스크 재시도·알림이 동작.
- ✅ **외부 스토리지 연동 완료**: 최소 1개 프로바이더(S3 우선)에 대해 업로드·다운로드·보존 정책 테스트 통과.
- ✅ **운영 가이드 완성도**: 신규 온보딩 엔지니어가 문서만으로 설치→성능 측정→비동기 태스크 실행을 재현 가능.

### 🔧 핵심 기술 스택 (제안)
- **비동기 처리**: Celery + Redis (분산 처리, 모니터링 지원)
- **외부 스토리지**: AWS S3 (확장성, boto3 라이브러리)
- **결과 저장**: PostgreSQL (index.json 대체) + Parquet (JSON 압축)
- **성능 최적화**: NumPy 벡터화 + Numba JIT (필요 시)
- **모니터링**: 구조화된 로깅 (JSON) + Celery Flower

### ⚠️ 주요 리스크 및 대응
1. **성능 최적화 실패**
   - 리스크: NumPy 벡터화가 기대치만큼 개선되지 않을 가능성
   - 대응: Numba JIT 옵션 준비, `PERFORMANCE_BENCHMARK_RESULTS.md`에 실행별 지표 기록해 병목을 조기 감지

2. **비동기 큐 안정성**
   - 리스크: Redis/Celery 장애, 태스크 유실, 재시도 누락
   - 대응: Redis persistence 활성화, `acks_late` + 재시도 파라미터 적용, Dead Letter Queue/Slack 알림 구성

3. **외부 스토리지 무결성**
   - 리스크: 업로드/다운로드 타이밍으로 데이터 손실·중복
   - 대응: 업로드 후 `ETag` 검증, 주기적 `list-compare` 스크립트, 버전 관리 버킷 사용

4. **PostgreSQL 마이그레이션 실패**
   - 리스크: 기존 `index.json` 데이터 손실 또는 롤백 불가
   - 대응: 마이그레이션 전 JSON 스냅샷 백업 + 복원 스크립트, Alembic 다운그레이드 리허설

5. **보안/비용 관리**
   - 리스크: S3 및 Redis 자원에 과다 권한 부여, 비용 급증
   - 대응: 최소 권한 IAM 정책, 비용 알람/리포트, 30일 만료 Lifecycle 정책

6. **프런트엔드/포지션 모델 불일치**
   - 리스크: 백엔드 스키마와 UI 렌더링 차이로 런타임 오류 발생
   - 대응: 계약 테스트(Schema validation) 추가, Storybook 기반 QA 데이터셋 공유

7. **운영 가이드 미완성**
   - 리스크: 인수인계 실패로 운영 공백
   - 대응: 설치→성능 측정→비동기 실행 시나리오 기반으로 작성, 신규 엔지니어 대상으로 파일럿 검증

### ✅ Task별 검증 플랜
| Task | 검증 항목 | 도구/방법 |
|------|-----------|-----------|
| 3.1 성능 최적화 | SLA 충족 여부, 회귀 발생 여부 | `scripts/test_performance_phase3.py`, 벤치마크 로그 비교 |
| 3.2 비동기 API | 제출/취소/조회 동작, 장애 복구 | Celery 통합 테스트, 브로커 장애 시뮬레이션, DLQ 로그 |
| 3.3 포지션 관리 | 백엔드 계산 정확도, UI 렌더링 | 백엔드 단위 테스트, 계약 테스트(JSON Schema), 프런트 스냅샷 |
| 3.4 외부 스토리지 | 업·다운로드 무결성, 권한 설정 | 통합 테스트, `ETag` 검증, IAM 정책 점검 |
| 3.5 결과 저장 | 마이그레이션 성공/롤백, Parquet 호환 | Alembic dry-run, Parquet 리더 테스트, 데이터 샘플 검증 |
| 3.6 운영 가이드 | 온보딩 재현성 | 체크리스트 기반 시나리오 실행, 신규 엔지니어 파일럿 |
| 3.7 백업/모니터링 | 백업 복구, 알림 정확도 | 정기 복구 리허설, 경보 페이로드 점검 |
| 3.8 통합 테스트 | e2e 품질, 성능/스토리지 회귀 | `pytest -m "phase3"`, 성능 및 스토리지 회귀 리포트 |

### 📦 예상 산출물 (26개)
- **코드**: 13개 (전략 최적화, Celery Task, Position 모델, S3 Provider, Frontend 컴포넌트 등)
- **문서**: 12개 (성능 리포트, API 구현 가이드, 운영 가이드, 완료 리포트 등)
- **설정**: 1개 (`.env.example`)

### 👥 인력 및 일정 가정
- 기본 가정: 엔지니어 2명이 병렬 작업하며 주당 4일 실개발 시간을 확보합니다.
- Week 1(3.1/3.2), Week 2(3.3/3.4)의 병행은 각 Task별 전담자가 있을 때만 유효합니다.
- 단일 인력 투입 시 전체 일정은 최소 +1.5주 증가하므로 High → Medium 순으로 선형 진행해야 합니다.
- 외부 의존(S3 권한, DBA 지원 등)이 지연될 경우 동일 주차의 다른 Task로 로테이션하는 것을 전제로 합니다.

### 🗓️ 권장 진행 순서
**Week 1** (High 우선순위)
- Day 1-4: Task 3.1 (성능 최적화) + Task 3.2 (비동기 API) 병렬 진행

**Week 2** (High + Medium)
- Day 1-2: Task 3.3 (포지션 관리) + Task 3.4 (외부 스토리지) 병렬 진행
- Day 3-4: Task 3.3 완료, Task 3.4 계속

**Week 3** (Medium + Low)
- Day 1-2: Task 3.4 완료
- Day 3: Task 3.5 (결과 저장 개선)
- Day 4: Task 3.6 (운영 가이드) 시작

**Week 4** (Low + 통합)
- Day 1-2: Task 3.6 완료
- Day 2-3: Task 3.7 (백업 및 모니터링)
- Day 4: Task 3.8 (통합 테스트 및 리포트)

---

## 4. 사용자 확인 요청

### 📌 핵심 확인 사항

Issue #29의 본문에는 매우 상세한 실행 계획이 포함되어 있습니다. 진행하기 전에 다음 사항을 확인해 주세요:

#### 질문 1: 기술 스택 선택 확인
- **비동기 처리**: Celery + Redis를 사용할까요? (대안: RQ, FastAPI BackgroundTasks)
- **외부 스토리지**: AWS S3를 사용할까요? (대안: NFS, OneDrive)
- **결과 저장**: PostgreSQL + Parquet로 전환할까요? (대안: MongoDB, JSON 유지)

#### 질문 2: 추가 작업 범위 확인
Issue 본문에서 제시된 5가지 질문에 대한 결정이 필요합니다:
1. **CI/CD 파이프라인**: Phase 3에서 GitHub Actions 구축이 필요한가요? (예: PR 시 자동 테스트)
2. **API 인증**: JWT 토큰 기반 인증을 Phase 3에서 구현할까요, Phase 4로 미룰까요?
3. **성능 모니터링**: Prometheus + Grafana를 Phase 3에서 도입할까요, Phase 4로 미룰까요?
4. **데이터베이스 마이그레이션**: Alembic 기반 마이그레이션 전략이 명확한가요?
5. **멀티 테넌시**: 여러 사용자 동시 사용을 고려해야 하나요?

#### 질문 3: 우선순위 조정
- 현재 계획은 **4주 (18-23일)**이 소요됩니다. 일정을 단축해야 하나요?
- 어떤 Task를 가장 먼저 진행하기를 원하나요? (권장: Task 3.1 성능 최적화)

#### 질문 4: 환경 준비 상태
- **AWS 계정**: S3 버킷 생성 및 IAM 권한 설정이 가능한가요?
- **Redis**: 이미 Docker Compose에 포함되어 있나요?
- **PostgreSQL**: 현재 사용 중인 데이터베이스가 있나요, 새로 구축해야 하나요?

### 🚀 제안: 다음 단계
1. **의사결정 세션(0.5일)**  
   - 위 4가지 질문에 대한 답을 확정하고, KPI/리소스/도구 선택을 회의록으로 남깁니다.  
   - 결정 내용은 `DECISION_REQUIRED.md`에 반영해 이후 변경 요청 시 기준으로 사용합니다.

2. **환경 준비 체크(0.5일)**  
   - S3 버킷/IAM, Redis, PostgreSQL, Celery Flower 등 외부 의존성을 실제로 기동해 보고 접속 정보(.env, Compose override)를 검증합니다.  
   - 미비 항목은 담당자/완료기한을 적어 Backlog로 이관합니다.

3. **Task 3.1 착수 (1-2일)**  
   - `scripts/test_performance_phase3.py` 실행 → `PERFORMANCE_BENCHMARK_RESULTS.md` 업데이트.  
   - 병목 구간을 확인 후 NumPy/Numba 실험을 적용하고, SLA 충족 여부를 `ri_18.md` 및 이슈 코멘트로 공유합니다.

4. **Task 3.2 준비 (병행)**  
   - Celery 워커/브로커를 스모크 테스트하고, 제출/조회/취소 API 스펙을 Swagger 또는 async 설계 문서에 정리합니다.  
   - 장애/재시도 시나리오를 QA 체크리스트에 추가해 후속 검증의 기준으로 삼습니다.

5. **주간 단위 점검 루틴 정착**  
   - 매주 말 KPI(테스트 통과 수, 성능 지표, 태스크 진척)를 측정해 표 형태로 이 문서에 업데이트합니다.  
   - 지연/리스크가 발생하면 다음 주 계획(3.1~3.8) 중 조정할 Task를 명시합니다.

**위 단계를 따라 진행해도 될까요? 필요한 조정 사항이 있으면 알려주세요.**

---

## 5. 즉시 실행 권장 순서
1. **확인 질문 답변 확정 및 기록**
   - 4장의 기술 스택/범위/환경 질문에 대해 결정권자의 합의를 받고, 결과를 `DECISION_REQUIRED.md`와 Issue 코멘트에 남깁니다.

2. **환경 검증 체크리스트 수행**
   - Celery 워커/브로커, S3 버킷+IAM, PostgreSQL 인스턴스를 실제로 기동·접속해 `.env`, `docker-compose.override.yml` 등에 반영합니다.
   - 미완료 항목은 담당자·기한을 명시해 Backlog로 이관합니다.

3. **Task 3.1 착수**
   - 최신 캔들 샘플로 `scripts/test_performance_phase3.py`를 실행하고, `PERFORMANCE_BENCHMARK_RESULTS.md`에 수치와 병목을 기록합니다.
   - 개선안(NumPy/Numba 등)을 시도한 뒤 SLA 충족 여부를 Issue #29 코멘트와 본 문서에 업데이트합니다.

4. **Task 3.2 준비 및 스모크 테스트**
   - Celery 제출/조회/취소 API 초안과 장애 시나리오를 설계 문서로 정리하고, 워커/브로커 스모크 테스트를 돌려 재시도·알림이 기본 동작하는지 확인합니다.

5. **주간 KPI 점검 루틴 수립**
   - 매주 금요일 기준으로 테스트 통과 수, 성능 지표, 태스크 진척도를 측정해 본 문서의 Task 표 하단에 주차별 로그를 추가합니다.
   - 지연 또는 리스크가 발생하면 다음 주 작업 순서를 재배치하고, 변경 사유/영향도를 기록합니다.
